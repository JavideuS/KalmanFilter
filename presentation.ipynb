{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kalman Filter and Regression Methods Explained\n",
    "\n",
    "This notebook combines code implementations with mathematical explanations using LaTeX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba183bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "Pkg.add([\"LinearAlgebra\", \"Plots\", \"DataFrames\", \"GLM\", \"StatsModels\", \"Statistics\", \"StatsPlots\", \"LaTeXStrings\"])\n",
    "using LinearAlgebra\n",
    "using Plots\n",
    "using GLM, DataFrames, StatsModels, Statistics, StatsPlots\n",
    "using LaTeXStrings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfc135a",
   "metadata": {},
   "source": [
    "## 1. Linear Regression\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "The basic linear regression problem can be formulated as:\n",
    "\n",
    "$$A = \\begin{pmatrix}\n",
    " 1 & x_0  \\\\\n",
    " \\vdots & \\vdots \\\\\n",
    "1 & x_n \\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "We solve the system:\n",
    "\n",
    "$$Ax = b \\rightarrow A^TAx=A^Tb \\rightarrow \\hat{x} = (A^TA)^{-1}A^Tb$$\n",
    "\n",
    "Where:\n",
    "- $A$ is the design matrix\n",
    "- $b$ is the vector of observations\n",
    "- $\\hat{x}$ contains the regression coefficients (intercept and slope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3779f99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "function linearRegression_compute(x::Vector{Float64}, y::Vector{Float64}, n::Int)\n",
    "    # Limit to n points\n",
    "    x_subset = x[1:n]\n",
    "    A = hcat(ones(n), x_subset)\n",
    "    y_subset = y[1:n]\n",
    "    A_inv = inv(A' * A)\n",
    "    Q = (A' * y_subset)\n",
    "    x_hat = A_inv * Q\n",
    "    return A_inv, x_hat, Q\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d382f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "function linearRegression_plot(x::Vector{Float64}, y::Vector{Float64}, n::Int)\n",
    "    # Get the computation result\n",
    "    covar, x_hat, Q = linearRegression_compute(x, y, n)\n",
    "    #covar, x_hat = randLinearRegression_compute(x, y, n)\n",
    "    \n",
    "    # Get the first n points for plotting\n",
    "    x_subset = x[1:n]\n",
    "    y_subset = y[1:n]\n",
    "    \n",
    "    p = scatter(x_subset, y_subset, label=\"Data\", title=\"Linear Regression (first $n points)\", \n",
    "                markersize=5, color=:blue, markerstrokewidth=0)\n",
    "    plot!(p, x_subset, x_hat[1] .+ x_hat[2] .* x_subset, label=\"Regression\", \n",
    "          color=:green, linewidth=2)\n",
    "    \n",
    "    return covar, x_hat,Q, p \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6272e28",
   "metadata": {},
   "source": [
    "### Testing Linear Regression\n",
    "\n",
    "Let's generate some test data and run the linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0577b902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test data\n",
    "points = rand(Float64, 16,2) * 10\n",
    "x = points[:,1]\n",
    "y = points[:,2]\n",
    "\n",
    "# Run linear regression on first 10 points\n",
    "n = 10\n",
    "#A_inv, x_hat, Q = linearRegression_compute(x, y, n)\n",
    "covar, x_hat,Q, p = linearRegression_plot(x, y, n)\n",
    "\n",
    "println(\"Regression coefficients:\")\n",
    "println(\"Intercept: \", x_hat[1])\n",
    "println(\"Slope: \", x_hat[2])\n",
    "\n",
    "# Plot linear regression result\n",
    "display(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0ffc67",
   "metadata": {},
   "source": [
    "## 2. Recursive Least Squares\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "The recursive least squares method updates the solution when new data points arrive. The key formulas are:\n",
    "\n",
    "$$(A^TA+r^Tr)x=A^Tb+r^Tb_{n+1}$$\n",
    "\n",
    "$$c = \\frac{1}{1+r(A^TA)^{-1}r^T}$$\n",
    "\n",
    "$$\\text{covar} = (A^TA)^{-1}$$\n",
    "\n",
    "$$(A^TA+r^Tr)^{-1}=(A^TA)^{-1}-c*(A^TA)^{-1}r^t*r(A^TA)^{-1}$$\n",
    "\n",
    "Where:\n",
    "- $r$ is the new data point\n",
    "- $c$ is the correction factor\n",
    "- $\\text{covar}$ is the covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415125bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "function recursiveRegression_compute(x::Vector{Float64}, y::Vector{Float64}, n::Int)\n",
    "    # Initial regression with first n points\n",
    "    covar, x_hat, Q = linearRegression_compute(x, y, n)\n",
    "\n",
    "    for iter in n+1:length(x)\n",
    "        x_new = [1; x[iter]]  # Column vector\n",
    "        y_new = y[iter]\n",
    "        \n",
    "        gain_vector = covar * x_new\n",
    "        c = 1 / (1 + (x_new' * gain_vector))\n",
    "        covar = covar - c * gain_vector * gain_vector'\n",
    "        Q = Q + (x_new * y_new)\n",
    "        x_hat = covar * Q\n",
    "    end\n",
    "    return covar, x_hat\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b10699",
   "metadata": {},
   "outputs": [],
   "source": [
    "function recursiveRegression_plot(x::Vector{Float64}, y::Vector{Float64}, n::Int)\n",
    "    # Initial regression with first n points\n",
    "    covar, x_hat, Q = linearRegression_compute(x, y, n)\n",
    "    \n",
    "    # Create first plot for initial fit\n",
    "    p1 = scatter(x[1:n], y[1:n], label=\"Data\", title=\"Linear Regression (first $n points)\", \n",
    "                markersize=5, color=:blue, markerstrokewidth=0)\n",
    "    plot!(p1, x[1:n], x_hat[1] .+ x_hat[2] .* x[1:n], label=\"Regression\", \n",
    "          color=:green, linewidth=2)\n",
    "\n",
    "    len = length(x)\n",
    "\n",
    "    p2 = plot(title=\"Recursive Least Squares Evolution\", legend=:outertopright)\n",
    "    scatter!(p2, x[1:n], y[1:n], label=\"Training Data (1-$n)\", \n",
    "             markersize=5, color=:blue, markerstrokewidth=0)\n",
    "    \n",
    "    # New points (n+1 to end) in orange\n",
    "    scatter!(p2, x[n+1:end], y[n+1:end], label=\"New Data ($(n+1)-$len)\", \n",
    "             markersize=5, color=:orange, markershape=:diamond, markerstrokewidth=0)\n",
    "\n",
    "    # Plot initial model\n",
    "    initial_x = range(minimum(x), maximum(x), length=100)\n",
    "    initial_line = x_hat[1] .+ x_hat[2] .* initial_x\n",
    "    plot!(p2, initial_x, initial_line, label=\"Initial (n=$n)\", linewidth=2, linestyle=:dash, markerstrokewidth=0)\n",
    "\n",
    "    for iter in n+1:length(x)\n",
    "        x_new = [1; x[iter]]  # Column vector\n",
    "        y_new = y[iter]\n",
    "        \n",
    "        gain_vector = covar * x_new\n",
    "        c = 1 / (1 + (x_new' * gain_vector))\n",
    "        covar = covar - c * gain_vector * x_new' * covar\n",
    "        error = y_new - (x_new' * x_hat)\n",
    "        \n",
    "        x_hat = x_hat + c * error * gain_vector\n",
    "        \n",
    "        # Plot this iteration's line with transparency based on iteration\n",
    "        alpha = 0.4 + 0.6 * (iter - n) / (len - n)  # Increasing opacity\n",
    "        iteration_line = x_hat[1] .+ x_hat[2] .* initial_x\n",
    "        if iter % 2 == 0  # Plot every two iterations\n",
    "            plot!(p2, initial_x, iteration_line, \n",
    "                  label=\"After point $iter\", \n",
    "                  linewidth=1.5,\n",
    "                  alpha=alpha)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    #y = b + mx\n",
    "    final_line = x_hat[1] .+ ( x_hat[2] .* initial_x)\n",
    "    plot!(p2, initial_x, final_line, label=\"Final model\", linewidth=3, color=:red)\n",
    "    \n",
    "    return covar, x_hat, p2\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4b8319",
   "metadata": {},
   "source": [
    "### Testing Recursive Least Squares\n",
    "\n",
    "Let's test the recursive implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2353f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run recursive regression\n",
    "#covar_recursive, x_hat_recursive = recursiveRegression_compute(x, y, n)\n",
    "covar_recursive, x_hat_recursive, p2 = recursiveRegression_plot(x, y, n)\n",
    "\n",
    "println(\"Recursive Regression coefficients:\")\n",
    "println(\"Intercept: \", x_hat_recursive[1])\n",
    "println(\"Slope: \", x_hat_recursive[2])\n",
    "\n",
    "# Plot recursive regression result\n",
    "display(p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27279431",
   "metadata": {},
   "source": [
    "## 3. Kalman Filter Regression\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "The Kalman (batch) filter approach uses:\n",
    "\n",
    "$$\\text{measurement\\_covar} := V = I(n) \\rightarrow n = \\text{points per batch}$$\n",
    "\n",
    "$$\\text{error\\_measurement\\_covar} := W_0 = (A_0^TV_0^{-1}A_0)^{-1} == covar^{-1}$$\n",
    "\n",
    "$$W_1^{-1} = W_0^{-1} + A_1^TV_1^{-1}A_1$$\n",
    "\n",
    "$$\\text{Kalman gain} := K_1 = W_1A_1^TV_1^{-1}$$\n",
    "\n",
    "$$\\hat{x}_1 = \\hat{x}_0 + K_1(b_1 - A_1 \\hat{x}_0)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b258a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "function CovarRecurrentRegression_compute(x::Vector{Float64}, y::Vector{Float64}, n::Int)\n",
    "    covar, x_hat, Q = linearRegression_compute(x, y, n)\n",
    "    \n",
    "    x_subset = x[n+1:end]\n",
    "    len = length(x_subset)\n",
    "    current_A = hcat(ones(len), x_subset)\n",
    "    y_subset = y[n+1:end]\n",
    "\n",
    "    inv_measurement_covar = I(len)  # Identity matrix as measurement covariance\n",
    "    error_covar = covar^-1\n",
    "    error_covar = error_covar + (current_A' * inv_measurement_covar * current_A)\n",
    "    kalman_gain = (error_covar^-1) * current_A' * inv_measurement_covar\n",
    "    x_hat = x_hat + kalman_gain * (y_subset - current_A * x_hat) \n",
    "    return error_covar, x_hat\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43990a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "function CovarRecurrentRegression_plot(x::Vector{Float64}, y::Vector{Float64}, n::Int)\n",
    "    # Call computation function\n",
    "    covar, x_hat = CovarRecurrentRegression_compute(x, y, n)\n",
    "    #covar, x_hat = randCovarRecurrentRegression_compute(x, y, n)\n",
    "    #covar, x_hat = KalmanRecurrentRegression_compute(x, y, n)\n",
    "    \n",
    "    #println(\"Kalman Filter Coefficients: Intercept: \", x_hat[1], \", Slope: \", x_hat[2])\n",
    "    \n",
    "    # Plotting\n",
    "    p = scatter(x, y, label=\"Data\", title=\"Kalman Filter Regression\", \n",
    "                markersize=5, color=:blue, markerstrokewidth=0)\n",
    "    plot!(p, x, x_hat[1] .+ x_hat[2] .* x, label=\"Kalman Filter Regression\", \n",
    "          color=:green, linewidth=2)\n",
    "          \n",
    "    return covar, x_hat, p\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60d0b75",
   "metadata": {},
   "source": [
    "### Testing Kalman Filter Regression\n",
    "\n",
    "Let's test the Kalman filter implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326391d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Kalman filter regression\n",
    "#error_covar, x_hat_kalman = CovarRecurrentRegression_compute(x, y, n)\n",
    "kalman_covar, x_hat_kalman, p_kalman = CovarRecurrentRegression_plot(x, y, n)\n",
    "\n",
    "println(\"Kalman Filter Regression coefficients:\")\n",
    "println(\"Intercept: \", x_hat_kalman[1])\n",
    "println(\"Slope: \", x_hat_kalman[2])\n",
    "\n",
    "\n",
    "\n",
    "display(p_kalman)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f4725c",
   "metadata": {},
   "source": [
    "## 4. Visualization and Comparison\n",
    "\n",
    "Let's create visualizations to compare all three methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6515dec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "function plotAllRegressionsDetailed(x::Vector{Float64}, y::Vector{Float64}, n::Int)\n",
    "      # 1. Linear\n",
    "      _, _, _, p1 = linearRegression_plot(x, y, n)\n",
    "      title!(p1, \"Linear Regression (first $n points)\")\n",
    "  \n",
    "      # 2. Recursive\n",
    "      _, _, p2 = recursiveRegression_plot(x, y, n)\n",
    "      title!(p2, \"Recursive Least Squares Evolution\")\n",
    "  \n",
    "      # 3. Kalman\n",
    "      _, _, p3 = CovarRecurrentRegression_plot(x, y, n)\n",
    "      title!(p3, \"Kalman Filter Regression\")\n",
    "  \n",
    "      # 4. Direct comparison of all three fits\n",
    "      p4 = plot(title=\"Comparison of All Methods\", legend=:outertopright)\n",
    "      scatter!(p4, x, y, label=\"All Data Points\", markersize=5, color=:black, alpha=0.6)\n",
    "  \n",
    "      # grab each method's coefficients again\n",
    "      covar_lin,     x_hat     , _ = linearRegression_compute(x, y, n)\n",
    "      covar_rec,     rec_hat   = recursiveRegression_compute(x, y, n)\n",
    "      covar_kalman,  kalman_hat = CovarRecurrentRegression_compute(x, y, n)\n",
    "  \n",
    "      # Print regression coefficients\n",
    "      println(\"\\nRegression Coefficients:\")\n",
    "      println(\"Method     | Intercept    | Slope\")\n",
    "      println(\"-----------|--------------|-------------\")\n",
    "      println(\"Linear     | $(round(x_hat[1], digits=4)) | $(round(x_hat[2], digits=4))\")\n",
    "      println(\"Recursive  | $(round(rec_hat[1], digits=4)) | $(round(rec_hat[2], digits=4))\")\n",
    "      println(\"Kalman     | $(round(kalman_hat[1], digits=4)) | $(round(kalman_hat[2], digits=4))\")\n",
    "  \n",
    "      plot_x = range(minimum(x), maximum(x), length=100)\n",
    "      plot!(p4, plot_x, x_hat[1] .+ x_hat[2] .* plot_x,       label=\"Linear (n=$n)\", linewidth=2)\n",
    "      plot!(p4, plot_x, rec_hat[1] .+ rec_hat[2] .* plot_x,   label=\"Recursive\",       linewidth=2)\n",
    "      plot!(p4, plot_x, kalman_hat[1] .+ kalman_hat[2] .* plot_x, label=\"Kalman\",       linewidth=2, linestyle=:dash)\n",
    "  \n",
    "      final_plot = plot(p1, p2, p3, p4, layout=(2,2), size=(1500,1500), margin=5Plots.mm)\n",
    "      return final_plot\n",
    "  end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e486bd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the comparison plot\n",
    "comparison_plot = plotAllRegressionsDetailed(x, y, n)\n",
    "display(comparison_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7245a85a",
   "metadata": {},
   "source": [
    "## 5. Performance Benchmarking\n",
    "\n",
    "Let's compare the performance of all three methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5166a3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "function benchmarkRegressionMethods(x::Vector{Float64}, y::Vector{Float64}, n::Int; runs::Int=100, warmup::Int=5)\n",
    "    # Prepare storage for timing results\n",
    "    times_linear = Float64[]\n",
    "    times_recursive = Float64[]\n",
    "    times_kalman = Float64[]\n",
    "    times_builtin = Float64[]\n",
    "    \n",
    "    # Prepare the design matrix for built-in regression once\n",
    "    X = hcat(ones(length(x)), x)\n",
    "    \n",
    "    # Run warm-up iterations to trigger JIT compilation\n",
    "    for i in 1:warmup\n",
    "        linearRegression_compute(x, y, n)\n",
    "        recursiveRegression_compute(x, y, n)\n",
    "        CovarRecurrentRegression_compute(x, y, n)\n",
    "        X \\ y\n",
    "    end\n",
    "    \n",
    "    # Main benchmark loop\n",
    "    for i in 1:runs\n",
    "        # Force GC before each measurement to reduce variability\n",
    "        GC.gc()\n",
    "        \n",
    "        # 1. Linear Regression\n",
    "        push!(times_linear, @elapsed linearRegression_compute(x, y, n))\n",
    "\n",
    "        #GC.gc()\n",
    "        \n",
    "        # 2. Recursive Least Squares\n",
    "        push!(times_recursive, @elapsed recursiveRegression_compute(x, y, n))\n",
    "        \n",
    "        #GC.gc()\n",
    "\n",
    "        # 3. Kalman Filter\n",
    "        push!(times_kalman, @elapsed CovarRecurrentRegression_compute(x, y, n))\n",
    "        #push!(times_kalman, @elapsed KalmanRecurrentRegression_compute(x, y, n))\n",
    "        \n",
    "        # 4. Built-in regression\n",
    "        push!(times_builtin, @elapsed X \\ y)\n",
    "    end\n",
    "    \n",
    "    # Calculate statistics\n",
    "    stats_df = DataFrame(\n",
    "        Method = [\"Linear\", \"Recursive\", \"Kalman\", \"Built-in\"],\n",
    "        Min_ms = [minimum(times_linear), minimum(times_recursive),\n",
    "                 minimum(times_kalman), minimum(times_builtin)] .* 1000,\n",
    "        Mean_ms = [mean(times_linear), mean(times_recursive),\n",
    "                  mean(times_kalman), mean(times_builtin)] .* 1000,\n",
    "        Median_ms = [median(times_linear), median(times_recursive),\n",
    "                    median(times_kalman), median(times_builtin)] .* 1000,\n",
    "        Max_ms = [maximum(times_linear), maximum(times_recursive),\n",
    "                 maximum(times_kalman), maximum(times_builtin)] .* 1000,\n",
    "        StdDev_ms = [std(times_linear), std(times_recursive),\n",
    "                    std(times_kalman), std(times_builtin)] .* 1000\n",
    "    )\n",
    "    \n",
    "    # Find the fastest method\n",
    "    fastest_idx = argmin(stats_df.Median_ms)\n",
    "    fastest_method = stats_df.Method[fastest_idx]\n",
    "    \n",
    "    # Print the usual benchmark table\n",
    "    println(\"\\n=== Performance Benchmark (over $runs runs with $warmup warmup iterations) ===\")\n",
    "    println(\"Method | Min (ms) | Mean (ms) | Median (ms) | Max (ms) | StdDev (ms)\")\n",
    "    println(\"---------|-----------|-----------|-------------|-----------|------------\")\n",
    "    for r in eachrow(stats_df)\n",
    "        println(\"$(lpad(r.Method,8)) | $(rpad(round(r.Min_ms, digits=3),9)) | \" *\n",
    "                \"$(rpad(round(r.Mean_ms, digits=3),9)) | \" *\n",
    "                \"$(rpad(round(r.Median_ms, digits=3),11)) | \" *\n",
    "                \"$(rpad(round(r.Max_ms, digits=3),9)) | \" *\n",
    "                \"$(round(r.StdDev_ms, digits=3))\")\n",
    "    end\n",
    "    \n",
    "    # Create enhanced box + violin plot\n",
    "    p = boxplot([\"Linear\" \"Recursive\" \"Kalman\" \"Built-in\"],\n",
    "              [times_linear .* 1000 times_recursive .* 1000 times_kalman .* 1000 times_builtin .* 1000],\n",
    "              title=\"Performance Comparison (lower is better)\",\n",
    "              xlabel=\"Method\", ylabel=\"Time (ms)\",\n",
    "              linewidth=1.5, fillalpha=0.75, outliers=true, legend=false,\n",
    "              xtickfontsize=10, ytickfontsize=10, titlefontsize=12)\n",
    "    \n",
    "    violin!(p, [\"Linear\" \"Recursive\" \"Kalman\" \"Built-in\"],\n",
    "          [times_linear .* 1000 times_recursive .* 1000 times_kalman .* 1000 times_builtin .* 1000],\n",
    "          alpha=0.3)\n",
    "    \n",
    "    # Force y-axis to start at zero for better comparison\n",
    "    ylims!(p, 0, maximum([maximum(times_linear), maximum(times_recursive), \n",
    "                         maximum(times_kalman), maximum(times_builtin)]) * 1000 * 1.1)\n",
    "    \n",
    "    # Performance analysis ratios\n",
    "    mean_times = stats_df.Mean_ms\n",
    "    linear_time = mean_times[1]\n",
    "    recursive_time = mean_times[2]\n",
    "    kalman_time = mean_times[3]\n",
    "    builtin_time = mean_times[4]\n",
    "    \n",
    "    # Corrected performance analysis\n",
    "    println(\"\\n=== Performance Analysis ===\")\n",
    "    recursive_kalman_ratio = kalman_time / recursive_time\n",
    "    if recursive_kalman_ratio < 1\n",
    "        println(\"- Kalman is $(round(1/recursive_kalman_ratio, digits=1))x faster than Recursive\")\n",
    "    else\n",
    "        println(\"- Recursive is $(round(recursive_kalman_ratio, digits=1))x faster than Kalman\")\n",
    "    end\n",
    "    \n",
    "    fastest_custom = minimum([recursive_time, kalman_time])\n",
    "    builtin_custom_ratio = builtin_time / fastest_custom\n",
    "    if builtin_custom_ratio < 1\n",
    "        println(\"- Built-in is $(round(1/builtin_custom_ratio, digits=1))x faster than the fastest custom method\")\n",
    "    else\n",
    "        println(\"- Fastest custom method is $(round(builtin_custom_ratio, digits=1))x faster than Built-in\")\n",
    "    end\n",
    "    \n",
    "    println(\"- Fastest method overall: $(fastest_method)\")\n",
    "    \n",
    "    # Normalize times relative to the fastest method\n",
    "    fastest_time = minimum(mean_times)\n",
    "    stats_df.Relative_Speed = stats_df.Mean_ms ./ fastest_time\n",
    "    \n",
    "    println(\"\\n=== Relative Performance ===\")\n",
    "    for r in eachrow(stats_df)\n",
    "        println(\"$(lpad(r.Method,8)): $(round(r.Relative_Speed, digits=2))x (relative to fastest)\")\n",
    "    end\n",
    "    \n",
    "    return p, stats_df\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239ccd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run benchmark\n",
    "benchmark_plot, stats = benchmarkRegressionMethods(x, y, n, runs=50)\n",
    "display(benchmark_plot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.5",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
